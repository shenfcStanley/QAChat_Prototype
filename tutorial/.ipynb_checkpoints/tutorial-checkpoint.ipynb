{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c11f76-4dec-4a0a-9bf7-30fc4941a692",
   "metadata": {},
   "source": [
    "*Author:* **Feichen Shen**  \n",
    "*Email:* shenfeichen1102@gmail.com \n",
    "*Date:* March 31, 2025"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "86d470e5-3180-457e-adcb-dcb9dbbf61e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data Loader\n",
    "from langchain_community.document_loaders import UnstructuredPDFLoader, PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "class DataReader:\n",
    "    def __init__(self, chunk_size=500, chunk_overlap=100):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "    def load_and_split(self, pdf_path):\n",
    "        \"\"\"Load and split a PDF into text chunks.\"\"\"\n",
    "        loader = UnstructuredPDFLoader(pdf_path, strategy=\"hi_res\")\n",
    "        documents = loader.load()\n",
    "\n",
    "        splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            chunk_overlap=self.chunk_overlap\n",
    "        )\n",
    " \n",
    "        split_docs = splitter.split_documents(documents)\n",
    "        print('doc', len(split_docs))\n",
    "        for i in range(len(split_docs)):\n",
    "            doc = split_docs[i]\n",
    "            page = doc.metadata.get(\"page\", \"?\")\n",
    "            doc.page_content = f\"{doc.page_content}\"\n",
    "\n",
    "        return split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "004616d8-7f46-40b1-bfc0-56884f851572",
   "metadata": {},
   "outputs": [],
   "source": [
    "## clinical knowledge embeddings\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import normalize\n",
    "from langchain.embeddings.base import Embeddings\n",
    "from langchain.schema import Document\n",
    "\n",
    "class ClinicalKGEmbedding(Embeddings):\n",
    "    def __init__(self, mapping_csv_path, embedding_pkl_path):\n",
    "        self.node_map = pd.read_csv(mapping_csv_path)\n",
    "        self.embedding_matrix = pickle.load(open(embedding_pkl_path, \"rb\")).numpy()\n",
    "        \n",
    "        # fast name lookup\n",
    "        self.name_to_idx = {\n",
    "            row['node_name'].lower(): row['global_graph_index']\n",
    "            for _, row in self.node_map.iterrows()\n",
    "        }\n",
    "        self.dim = self.embedding_matrix.shape[1]\n",
    "\n",
    "    def _embed_text(self, text: str):\n",
    "        text = text.lower()\n",
    "        matched = [self.embedding_matrix[idx]\n",
    "                   for name, idx in self.name_to_idx.items()\n",
    "                   if name in text]\n",
    "\n",
    "        if matched:\n",
    "            avg_vec = np.mean(matched, axis=0)\n",
    "        else:\n",
    "            avg_vec = np.zeros(self.dim)\n",
    "\n",
    "        return normalize(avg_vec.reshape(1, -1))[0]\n",
    "\n",
    "    def embed_documents(self, texts):\n",
    "        return [self._embed_text(doc.page_content if isinstance(doc, Document) else doc) for doc in texts]\n",
    "\n",
    "    def embed_query(self, text):\n",
    "        return self._embed_text(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5fa4e2e2-ce0b-4e69-aefb-002f44c0db32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_community.llms import LlamaCpp\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from difflib import SequenceMatcher\n",
    "import os\n",
    "from langchain.vectorstores import FAISS\n",
    "\n",
    "os.environ[\"OCR_AGENT\"] = \"tesseract\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3c0cc057-d28c-4414-ac9b-e32c64586c22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doc 54\n",
      "Loaded 54 chunks from the PDF.\n",
      "ResearchGate\n",
      "\n",
      "See discussions, stats, and author profiles for this publication at: https://www.researchgate.net/publication/19364043\n",
      "\n",
      "Slamon DJ, Clark GM, Wong SG, Levin WJ, Ullrich A, McGuire WLHuman breast cancer: correlation of relapse and survival with amplification of the HER-2/neu oncogene. Science (Wash DC...\n",
      "\n",
      "Article in Science - February 1987 DOI: 10.1126/science.3798106 - Source: PubMed\n",
      "\n",
      "CITATIONS\n",
      "\n",
      "READS\n",
      "\n",
      "9,978\n",
      "\n",
      "29,134\n",
      "\n",
      "6 authors, including:\n",
      "\n",
      "Gary Clark\n",
      "\n",
      "Gary Clark Statistical Consulting, LLC\n",
      "\n",
      "Steven G Wong University of California, Los Angeles\n",
      "\n",
      "364 PUBLICATIONS 69,847 CITATIONS\n",
      "\n",
      "24 PUBLICATIONS 24,981 CITATIONS\n",
      "\n",
      "SEE PROFILE\n",
      "\n",
      "SEE PROFILE\n",
      "\n",
      "Wendy Levin\n",
      "\n",
      "Fate Therapeutics, Inc.\n",
      "\n",
      "25 PUBLICATIONS 17,968 CITATIONS\n",
      "\n",
      "SEE PROFILE\n",
      "\n",
      "All content following this page was uploaded by Gary Clark on 23 December 2013.\n",
      "\n",
      "The user has requested enhancement of the downloaded file.\n",
      "\n",
      "BIOM 255 (Leffert) â€” Discussion Feb. 1, 2007\n",
      "\n",
      "\n",
      "\n",
      "scover.bio-rad.com\n",
      "\n",
      "Science AVAAAS\n"
     ]
    }
   ],
   "source": [
    "def find_best_sentence(answer, source_docs):\n",
    "    best_match = \"\"\n",
    "    best_score = 0\n",
    "    best_meta = {}\n",
    "\n",
    "    for doc in source_docs:\n",
    "        sentences = doc.page_content.split(\". \")\n",
    "        for sent in sentences:\n",
    "            score = SequenceMatcher(None, sent.lower(), answer.lower()).ratio()\n",
    "            if score > best_score:\n",
    "                best_match = sent.strip()\n",
    "                best_meta = doc.metadata\n",
    "                best_score = score\n",
    "    return best_match, best_meta, best_score\n",
    "\n",
    "def shorten_sentence(text, max_words=30):\n",
    "    words = text.split()\n",
    "    return \" \".join(words[:max_words]) + (\"...\" if len(words) > max_words else \"\")\n",
    "\n",
    "pdf_path = \"../data/HER2_Paper.pdf\"\n",
    "processor = DataReader(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = processor.load_and_split(pdf_path)\n",
    "\n",
    "print(f\"Loaded {len(chunks)} chunks from the PDF.\")\n",
    "print(chunks[0].page_content)  # Example output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4278c61c-fcf7-4c17-a25e-3b7990f900c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment the following code to use MiniLM\n",
    "## embedding_model = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n",
    "\n",
    "# uncomment the following code to use Bio_ClinicalBERT\n",
    "## embedding_model = HuggingFaceEmbeddings(model_name=\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "## Use pre-trained Clinical Knowledge Embeddings\n",
    "embedding_model = ClinicalKGEmbedding(\n",
    "        mapping_csv_path=\"../clinical_KGEmb/new_node_map_df.csv\",\n",
    "        embedding_pkl_path=\"../clinical_KGEmb/full_h_embed_hms.pkl\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "20d7c34d-7f59-4bab-8883-fca3e4ca5b45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retriever: tags=['FAISS', 'ClinicalKGEmbedding'] vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002AE5F9ACC50> search_kwargs={'k': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 22 key-value pairs and 291 tensors from ../models/Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
      "llama_model_loader: - kv   1:                               general.name str              = models\n",
      "llama_model_loader: - kv   2:                       llama.context_length u32              = 32768\n",
      "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
      "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
      "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n",
      "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
      "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 8\n",
      "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  11:                          general.file_type u32              = 15\n",
      "llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32002]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32002]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
      "llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32002]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  18:               tokenizer.ggml.add_bos_token bool             = true\n",
      "llama_model_loader: - kv  19:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  20:                    tokenizer.chat_template str              = {% for message in messages %}{{'<|im_...\n",
      "llama_model_loader: - kv  21:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   65 tensors\n",
      "llama_model_loader: - type q4_K:  193 tensors\n",
      "llama_model_loader: - type q6_K:   33 tensors\n",
      "print_info: file format = GGUF V3 (latest)\n",
      "print_info: file type   = Q4_K - Medium\n",
      "print_info: file size   = 4.07 GiB (4.83 BPW) \n",
      "init_tokenizer: initializing tokenizer for type 1\n",
      "load: control-looking token:  32000 '<|im_end|>' was not control-type; this is probably a bug in the model. its type will be overridden\n",
      "load: control token:      2 '</s>' is not marked as EOG\n",
      "load: control token:      1 '<s>' is not marked as EOG\n",
      "load: special tokens cache size = 5\n",
      "load: token to piece cache size = 0.1637 MB\n",
      "print_info: arch             = llama\n",
      "print_info: vocab_only       = 0\n",
      "print_info: n_ctx_train      = 32768\n",
      "print_info: n_embd           = 4096\n",
      "print_info: n_layer          = 32\n",
      "print_info: n_head           = 32\n",
      "print_info: n_head_kv        = 8\n",
      "print_info: n_rot            = 128\n",
      "print_info: n_swa            = 0\n",
      "print_info: n_embd_head_k    = 128\n",
      "print_info: n_embd_head_v    = 128\n",
      "print_info: n_gqa            = 4\n",
      "print_info: n_embd_k_gqa     = 1024\n",
      "print_info: n_embd_v_gqa     = 1024\n",
      "print_info: f_norm_eps       = 0.0e+00\n",
      "print_info: f_norm_rms_eps   = 1.0e-05\n",
      "print_info: f_clamp_kqv      = 0.0e+00\n",
      "print_info: f_max_alibi_bias = 0.0e+00\n",
      "print_info: f_logit_scale    = 0.0e+00\n",
      "print_info: f_attn_scale     = 0.0e+00\n",
      "print_info: n_ff             = 14336\n",
      "print_info: n_expert         = 0\n",
      "print_info: n_expert_used    = 0\n",
      "print_info: causal attn      = 1\n",
      "print_info: pooling type     = 0\n",
      "print_info: rope type        = 0\n",
      "print_info: rope scaling     = linear\n",
      "print_info: freq_base_train  = 10000.0\n",
      "print_info: freq_scale_train = 1\n",
      "print_info: n_ctx_orig_yarn  = 32768\n",
      "print_info: rope_finetuned   = unknown\n",
      "print_info: ssm_d_conv       = 0\n",
      "print_info: ssm_d_inner      = 0\n",
      "print_info: ssm_d_state      = 0\n",
      "print_info: ssm_dt_rank      = 0\n",
      "print_info: ssm_dt_b_c_rms   = 0\n",
      "print_info: model type       = 7B\n",
      "print_info: model params     = 7.24 B\n",
      "print_info: general.name     = models\n",
      "print_info: vocab type       = SPM\n",
      "print_info: n_vocab          = 32002\n",
      "print_info: n_merges         = 0\n",
      "print_info: BOS token        = 1 '<s>'\n",
      "print_info: EOS token        = 32000 '<|im_end|>'\n",
      "print_info: EOT token        = 32000 '<|im_end|>'\n",
      "print_info: UNK token        = 0 '<unk>'\n",
      "print_info: LF token         = 13 '<0x0A>'\n",
      "print_info: EOG token        = 32000 '<|im_end|>'\n",
      "print_info: max token length = 48\n",
      "load_tensors: loading model tensors, this can take a while... (mmap = true)\n",
      "load_tensors: layer   0 assigned to device CPU\n",
      "load_tensors: layer   1 assigned to device CPU\n",
      "load_tensors: layer   2 assigned to device CPU\n",
      "load_tensors: layer   3 assigned to device CPU\n",
      "load_tensors: layer   4 assigned to device CPU\n",
      "load_tensors: layer   5 assigned to device CPU\n",
      "load_tensors: layer   6 assigned to device CPU\n",
      "load_tensors: layer   7 assigned to device CPU\n",
      "load_tensors: layer   8 assigned to device CPU\n",
      "load_tensors: layer   9 assigned to device CPU\n",
      "load_tensors: layer  10 assigned to device CPU\n",
      "load_tensors: layer  11 assigned to device CPU\n",
      "load_tensors: layer  12 assigned to device CPU\n",
      "load_tensors: layer  13 assigned to device CPU\n",
      "load_tensors: layer  14 assigned to device CPU\n",
      "load_tensors: layer  15 assigned to device CPU\n",
      "load_tensors: layer  16 assigned to device CPU\n",
      "load_tensors: layer  17 assigned to device CPU\n",
      "load_tensors: layer  18 assigned to device CPU\n",
      "load_tensors: layer  19 assigned to device CPU\n",
      "load_tensors: layer  20 assigned to device CPU\n",
      "load_tensors: layer  21 assigned to device CPU\n",
      "load_tensors: layer  22 assigned to device CPU\n",
      "load_tensors: layer  23 assigned to device CPU\n",
      "load_tensors: layer  24 assigned to device CPU\n",
      "load_tensors: layer  25 assigned to device CPU\n",
      "load_tensors: layer  26 assigned to device CPU\n",
      "load_tensors: layer  27 assigned to device CPU\n",
      "load_tensors: layer  28 assigned to device CPU\n",
      "load_tensors: layer  29 assigned to device CPU\n",
      "load_tensors: layer  30 assigned to device CPU\n",
      "load_tensors: layer  31 assigned to device CPU\n",
      "load_tensors: layer  32 assigned to device CPU\n",
      "load_tensors: tensor 'token_embd.weight' (q4_K) (and 290 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n",
      "load_tensors:   CPU_Mapped model buffer size =  4165.38 MiB\n",
      "................................................................................................\n",
      "llama_init_from_model: n_batch is less than GGML_KQ_MASK_PAD - increasing to 64\n",
      "llama_init_from_model: n_seq_max     = 1\n",
      "llama_init_from_model: n_ctx         = 2048\n",
      "llama_init_from_model: n_ctx_per_seq = 2048\n",
      "llama_init_from_model: n_batch       = 64\n",
      "llama_init_from_model: n_ubatch      = 8\n",
      "llama_init_from_model: flash_attn    = 0\n",
      "llama_init_from_model: freq_base     = 10000.0\n",
      "llama_init_from_model: freq_scale    = 1\n",
      "llama_init_from_model: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_init: kv_size = 2048, offload = 1, type_k = 'f16', type_v = 'f16', n_layer = 32, can_shift = 1\n",
      "llama_kv_cache_init: layer 0: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 1: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 2: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 3: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 4: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 5: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 6: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 7: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 8: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 9: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 10: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 11: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 12: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 13: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 14: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 15: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 16: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 17: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 18: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 19: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 20: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 21: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 22: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 23: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 24: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 25: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 26: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 27: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 28: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 29: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 30: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init: layer 31: n_embd_k_gqa = 1024, n_embd_v_gqa = 1024\n",
      "llama_kv_cache_init:        CPU KV buffer size =   256.00 MiB\n",
      "llama_init_from_model: KV self size  =  256.00 MiB, K (f16):  128.00 MiB, V (f16):  128.00 MiB\n",
      "llama_init_from_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_init_from_model:        CPU compute buffer size =     3.00 MiB\n",
      "llama_init_from_model: graph nodes  = 1030\n",
      "llama_init_from_model: graph splits = 1\n",
      "CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n",
      "Model metadata: {'general.name': 'models', 'general.architecture': 'llama', 'llama.context_length': '32768', 'llama.rope.dimension_count': '128', 'llama.embedding_length': '4096', 'llama.block_count': '32', 'llama.feed_forward_length': '14336', 'llama.attention.head_count': '32', 'tokenizer.ggml.eos_token_id': '32000', 'general.file_type': '15', 'llama.attention.head_count_kv': '8', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.freq_base': '10000.000000', 'tokenizer.ggml.model': 'llama', 'general.quantization_version': '2', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.add_bos_token': 'true', 'tokenizer.ggml.add_eos_token': 'false', 'tokenizer.chat_template': \"{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}\"}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Guessed chat format: chatml\n"
     ]
    }
   ],
   "source": [
    "# vector db\n",
    "db = FAISS.from_documents(chunks, embedding_model)\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 4})\n",
    "print(\"Retriever:\", retriever)\n",
    "\n",
    "llm = LlamaCpp(\n",
    "    #model_path=\"models/llama-2-7b.Q4_K_M.gguf\",\n",
    "    model_path=\"../models/Nous-Hermes-2-Mistral-7B-DPO.Q4_K_M.gguf\",\n",
    "    n_ctx=2048,\n",
    "    temperature=0.1,\n",
    "    max_tokens=512\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "727b2307-67dd-4e49-a4a7-5b627e563388",
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"\n",
    "You are a helpful assistant answering questions about HER2 from a scientific paper.\n",
    "Use the provided context to answer the question. \n",
    "Include the exact sentence used in your answer in brackets.\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "Answer:\"\"\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5dfd0db2-cda8-454b-b085-24e5b6bca9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_chain = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,\n",
    "    chain_type=\"stuff\",\n",
    "    chain_type_kwargs={\"prompt\": custom_prompt},\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bd9019a6-00e8-4ed9-8108-19e489d4b165",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_perf_context_print:        load time =  173374.34 ms\n",
      "llama_perf_context_print: prompt eval time =  173373.76 ms /   877 tokens (  197.69 ms per token,     5.06 tokens per second)\n",
      "llama_perf_context_print:        eval time =   11096.97 ms /    41 runs   (  270.66 ms per token,     3.69 tokens per second)\n",
      "llama_perf_context_print:       total time =  184634.51 ms /   918 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Answer:\n",
      "\n",
      " The text suggests that HER2 may play a role in the biologic behavior and/or pathogenesis of human breast cancer. It is also associated with disease relapse and overall patient survival.\n",
      "\n",
      " Source Evidence from Paper:\n",
      "len of souce_doc 4\n",
      "1. \" prognostic factors, including hormonal-receptor status, in lymph node-positive disease. These data indicate that this gene may play a role in the biologic behavior and/or pathogenesis o! of human bi breast cancer....\"\n",
      "\n",
      "2. \" Recently, a novel transforming gene was identified as a result of transfection studies with DNA from chemically induced rat neu- roglioblastomas (20). This gene, called new, was shown to be related to, but distinct from, the c-erbB proto-oncogene (21). By means of v-erbB and human EGFR as probes to screen human genomic and complementary DNA (cDNA) libraries, two other groups indepen- dently isolat...\"\n",
      "\n",
      "3. \" Of 103 tumors evaluated in the initial survey, there was essentially no correlation between gene amplification and estrogen receptor status, progesterone receptor status, size of tumors, or age at diagnosis (Table 1). However, when analysis was performed for association between HER-2/neu amplification and number of posi- tive lymph nodes, a trend was noted. This analysis showed that 4/34 (11%) of...\"\n",
      "\n",
      "4. \" *To whom correspondence should be addressed.\n",
      "\n",
      "9 JANUARY 1987\n",
      "\n",
      "ARTICLES 177\n",
      "\n",
      "Downloaded from www.sciencemag.org on January 15, 2007\n",
      "\n",
      "BIOM 255 (Leffert) - Discussion Feb. 1, 2007\n",
      "\n",
      "that it too is likely to be a cellular receptor for an as yet unidentified ligand.\n",
      "\n",
      "Table 1. Association between HER-2/neu amplification and disease parame- ters in 103 breast tumors.\n",
      "\n",
      "As a result of the published data sho...\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "query = \"What is the role of HER2 in breast cancer?\"\n",
    "result = qa_chain.invoke(query)\n",
    "\n",
    "best_sentence, meta, score = find_best_sentence(result[\"result\"], result[\"source_documents\"])\n",
    "short_snippet = shorten_sentence(best_sentence)\n",
    "\n",
    "print(\"Answer:\\n\")\n",
    "print(result[\"result\"])\n",
    "\n",
    "print(\"\\n Source Evidence from Paper:\")\n",
    "print('len of souce_doc', len(result[\"source_documents\"]))\n",
    "for i, doc in enumerate(result[\"source_documents\"], start=1):\n",
    "    page = doc.metadata.get(\"page\", \"?\")\n",
    "    snippet = doc.page_content[:400].strip()\n",
    "    print(f'{i}. \" {snippet}...\"')\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab89377-fedc-4e6c-8110-df9206240aaa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
